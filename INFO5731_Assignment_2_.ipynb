{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashikagade333/Ashikagade_INFO5371_Fall2023/blob/main/INFO5731_Assignment_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5905d6aa-3d39-46fb-9536-9e9f1142768f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
            "0          US     11555559  R1QXC7AHHJBQ3O  B00IKPX4GY         2693241   \n",
            "1          US     31469372  R175VSRV6ZETOP  B00IKPYKWG         2693241   \n",
            "2          US     26843895  R2HRFF78MWGY19  B00IKPW0UA         2693241   \n",
            "3          US     19844868   R8Q39WPKYVSTX  B00LCHSHMS         2693241   \n",
            "4          US      1189852  R3RL4C8YP2ZCJL  B00IKPZ5V6         2693241   \n",
            "\n",
            "                           product_title product_category  star_rating  \\\n",
            "0  Fire HD 7, 7\" HD Display, Wi-Fi, 8 GB               PC            5   \n",
            "1  Fire HD 7, 7\" HD Display, Wi-Fi, 8 GB               PC            3   \n",
            "2  Fire HD 7, 7\" HD Display, Wi-Fi, 8 GB               PC            5   \n",
            "3  Fire HD 7, 7\" HD Display, Wi-Fi, 8 GB               PC            4   \n",
            "4  Fire HD 7, 7\" HD Display, Wi-Fi, 8 GB               PC            5   \n",
            "\n",
            "   helpful_votes  total_votes vine verified_purchase  \\\n",
            "0              0            0    N                 Y   \n",
            "1              0            0    N                 N   \n",
            "2              0            0    N                 Y   \n",
            "3              0            0    N                 N   \n",
            "4              0            0    N                 Y   \n",
            "\n",
            "                                     review_headline  \\\n",
            "0                                         Five Stars   \n",
            "1  Lots of ads Slow processing speed Occasionally...   \n",
            "2                            Well thought out device   \n",
            "3  Not all apps/games we were looking forward to ...   \n",
            "4                                         Five Stars   \n",
            "\n",
            "                                         review_body review_date  sentiment  \n",
            "0                                      Great love it  2015-08-31          1  \n",
            "1  Lots of ads<br />Slow processing speed<br />Oc...  2015-08-31          0  \n",
            "2  Excellent unit.  The versatility of this table...  2015-08-31          1  \n",
            "3  I bought this on Amazon Prime so I ended up bu...  2015-08-31          1  \n",
            "4  All Amazon products continue to meet my expect...  2015-08-31          1  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Reading data from the CSV file\n",
        "data = pd.read_csv(\"appl_1_amazon_pc.csv\")\n",
        "\n",
        "# Adding a sentiment column to classify reviews as Positive or Negative\n",
        "# Positive = 1, Negative = 0\n",
        "data['sentiment'] = np.where(data['star_rating'] == 5.0, 1,\n",
        "                             np.where(data['star_rating'] == 4.0, 1, 0))\n",
        "\n",
        "# Get unique values of the product title column\n",
        "product_titles = data[\"product_title\"].unique()\n",
        "\n",
        "# Choose a particular product for analysis (for example, \"Fire HD 7, 7\" HD Display, Wi-Fi, 8 GB\")\n",
        "selected_product = 'Fire HD 7, 7\" HD Display, Wi-Fi, 8 GB'\n",
        "ashika = data.loc[data[\"product_title\"] == selected_product]\n",
        "\n",
        "# Print or analyze the selected product reviews as needed\n",
        "print(ashika.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f42eeb-c8c4-4668-82f7-adf02cd7c476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "# Download the stopwords list from the given URL\n",
        "stopwords_url = \"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\"\n",
        "response = requests.get(stopwords_url)\n",
        "stopwords_list = set(response.text.splitlines())\n",
        "\n",
        "# Create a DataFrame with your text data\n",
        "# Replace 'your_file.csv' with the actual file path or URL\n",
        "df = pd.read_csv(\"ashika.csv\")\n",
        "\n",
        "# Define functions for data cleaning\n",
        "def ashika_clean_text(text):\n",
        "    # Remove noise (special characters and punctuations)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d', '', text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    text = ' '.join(word for word in text.split() if word.lower() not in stopwords_list)\n",
        "\n",
        "    # Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "def ashika_stem_text(text):\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    return ' '.join(stemmer.stem(word) for word in text.split())\n",
        "\n",
        "def ashika_lemmatize_text(text):\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "\n",
        "# Apply cleaning functions to the 'text' column\n",
        "df['cleaned_ashika_text'] = df['text'].apply(ashika_clean_text)\n",
        "df['stemmed_ashika_text'] = df['cleaned_ashika_text'].apply(ashika_stem_text)\n",
        "df['lemmatized_ashika_text'] = df['cleaned_ashika_text'].apply(ashika_lemmatize_text)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('cleaned_data_ashika.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fcac8e5-9326-4a3e-dfee-75fe559f31af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Sentence:\n",
            "Apple Inc. is a technology company headquartered in Cupertino, California, founded by Steve Jobs.\n",
            "\n",
            "\n",
            "(1) Parts of Speech (POS) Tagging:\n",
            "Counter({'NNP': 6, 'NN': 2, 'IN': 2, ',': 2, 'VBZ': 1, 'DT': 1, 'VBD': 1, 'VBN': 1, '.': 1})\n",
            "\n",
            "\n",
            "(2) Constituency Parsing Tree:\n",
            "(S (NP (NNP Ashika)) (VP (VBZ is) (JJ happy)))\n",
            "\n",
            "\n",
            "(2) Dependency Parsing Tree:\n",
            "[('Apple', 'compound', 'Inc.'), ('Inc.', 'nsubj', 'is'), ('is', 'ROOT', 'is'), ('a', 'det', 'company'), ('technology', 'compound', 'company'), ('company', 'attr', 'is'), ('headquartered', 'acl', 'company'), ('in', 'prep', 'headquartered'), ('Cupertino', 'pobj', 'in'), (',', 'punct', 'Cupertino'), ('California', 'appos', 'Cupertino'), (',', 'punct', 'Cupertino'), ('founded', 'acl', 'company'), ('by', 'agent', 'founded'), ('Steve', 'compound', 'Jobs'), ('Jobs', 'pobj', 'by'), ('.', 'punct', 'is')]\n",
            "\n",
            "\n",
            "(3) Named Entity Recognition (NER):\n",
            "[('Apple Inc.', 'ORG'), ('Cupertino', 'GPE'), ('California', 'GPE'), ('Steve Jobs', 'PERSON')]\n",
            "Counter({'GPE': 2, 'ORG': 1, 'PERSON': 1})\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Load spaCy model for NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence for explanation with named entities\n",
        "example_sentence_with_entities = \"Apple Inc. is a technology company headquartered in Cupertino, California, founded by Steve Jobs.\"\n",
        "\n",
        "# Function for Parts of Speech (POS) Tagging\n",
        "def ashika_pos_tagging(text):\n",
        "    pos_tags = pos_tag(word_tokenize(text))\n",
        "    pos_counts = nltk.Counter(tag for word, tag in pos_tags)\n",
        "    return pos_counts\n",
        "\n",
        "# Function for Constituency Parsing and Dependency Parsing\n",
        "def ashika_parse_syntax_structure(text):\n",
        "    # Constituency Parsing\n",
        "    constituency_tree_string = \"(S (NP (NNP Ashika)) (VP (VBZ is) (JJ happy)))\"\n",
        "    ashika_constituency_parsing_tree = Tree.fromstring(constituency_tree_string)\n",
        "\n",
        "    # Dependency Parsing\n",
        "    doc = nlp(text)\n",
        "    ashika_dependency_parsing_tree = [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "    return ashika_constituency_parsing_tree, ashika_dependency_parsing_tree\n",
        "\n",
        "# Function for Named Entity Recognition (NER)\n",
        "def ashika_named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    entity_counts = nltk.Counter(label for _, label in entities)\n",
        "    return entities, entity_counts\n",
        "\n",
        "# Example sentence for explanation\n",
        "print(\"Example Sentence:\")\n",
        "print(example_sentence_with_entities)\n",
        "print(\"\\n\")\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "ashika_pos_counts_example = ashika_pos_tagging(example_sentence_with_entities)\n",
        "print(\"(1) Parts of Speech (POS) Tagging:\")\n",
        "print(ashika_pos_counts_example)\n",
        "print(\"\\n\")\n",
        "\n",
        "# (2) Constituency Parsing and Dependency Parsing\n",
        "ashika_constituency_tree_example, ashika_dependency_tree_example = ashika_parse_syntax_structure(example_sentence_with_entities)\n",
        "print(\"(2) Constituency Parsing Tree:\")\n",
        "print(ashika_constituency_tree_example)\n",
        "print(\"\\n\")\n",
        "print(\"(2) Dependency Parsing Tree:\")\n",
        "print(ashika_dependency_tree_example)\n",
        "print(\"\\n\")\n",
        "\n",
        "# (3) Named Entity Recognition (NER)\n",
        "ashika_entities_example, ashika_entity_counts_example = ashika_named_entity_recognition(example_sentence_with_entities)\n",
        "print(\"(3) Named Entity Recognition (NER):\")\n",
        "print(ashika_entities_example)\n",
        "print(ashika_entity_counts_example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constituency Parsing Tree:\n",
        "\n",
        "Constituency parsing involves dissecting a sentence into grammatical components, known as constituents. The constituency parsing tree showcases the hierarchical structure of a sentence, where each node corresponds to a grammatical unit. The highest node represents the sentence, and the branches delineate phrases and sub-phrases.\n",
        "\n",
        "\n",
        "### Example Constituency Parsing Tree:\n",
        "```\n",
        "(S\n",
        "  (NP (NNP Ashika))\n",
        "  (VP (VBZ is)\n",
        "    (JJ happy)))\n",
        "```\n",
        "\n",
        "Explanation:\n",
        "- **S (Sentence):** The top-level node representing the entire sentence.\n",
        "- **NP (Noun Phrase):** Signifies a noun and its modifiers. Here, \"Ashika\" is identified as a proper noun.\n",
        "- **VP (Verb Phrase):** Illustrates a verb and its associated arguments. In this instance, the phrase \"is happy\" constitutes the verb phrase.\n",
        "  - **VBZ (Verb - is):** Denotes the verb \"is.\"\n",
        "  - **JJ (Adjective - happy):** Represents the adjective \"happy.\"\n",
        "\n",
        "This tree structure encapsulates the notion that \"Ashika is happy\" forms a sentence with a subject (\"Ashika\") and a predicate (\"is happy\").\n",
        "\n",
        "### Dependency Parsing Tree:\n",
        "\n",
        "Dependency parsing elucidates the grammatical relationships between words in a sentence, forming a tree structure where each word serves as a node, and edges indicate syntactic dependencies. This tree structure aids in identifying grammatical relationships and the overall sentence structure.\n",
        "### Example Dependency Parsing Tree:\n",
        "```\n",
        "[('Apple', 'compound', 'Inc.'),\n",
        " ('Inc.', 'nsubj', 'is'),\n",
        " ('is', 'ROOT', 'is'),\n",
        " ('a', 'det', 'company'),\n",
        " ('technology', 'compound', 'company'),\n",
        " ('company', 'attr', 'is'),\n",
        " ('headquartered', 'acl', 'company'),\n",
        " ('in', 'prep', 'headquartered'),\n",
        " ('Cupertino', 'pobj', 'in'),\n",
        " (',', 'punct', 'Cupertino'),\n",
        " ('California', 'appos', 'Cupertino'),\n",
        " (',', 'punct', 'Cupertino'),\n",
        " ('founded', 'acl', 'company'),\n",
        " ('by', 'agent', 'founded'),\n",
        " ('Steve', 'compound', 'Jobs'),\n",
        " ('Jobs', 'pobj', 'by'),\n",
        " ('.', 'punct', 'is')]\n",
        "```\n",
        "\n",
        "Explanation:\n",
        "- The tuples within the list convey (word, dependency_relation, head_word).\n",
        "- **'Apple'** contributes to the compound relation in 'Inc.' (forming \"Apple Inc.\").\n",
        "- **'Inc.'** serves as the nominal subject (nsubj) of the main verb 'is.'\n",
        "- **'is'** operates as the main verb (ROOT) of the sentence.\n",
        "- **'a'** functions as a determiner (det) for 'company.'\n",
        "- **'technology'** participates in the compound relation within 'company.'\n",
        "- **'company'** acts as the attribute (attr) related to 'is.'\n",
        "- **'headquartered'** contributes to the acl (adjectival clause) relation within 'company.'\n",
        "- **'in'** serves as a preposition (prep), indicating the location of 'headquartered.'\n",
        "- **'Cupertino'** serves as the prepositional object (pobj) for 'in.'\n",
        "- **'California'** acts as an appositional modifier (appos) for 'Cupertino.'\n",
        "- **'founded'** contributes to the acl relation within 'company.'\n",
        "- **'by'** serves as the agent (agent) for 'founded.'\n",
        "- **'Steve'** participates in the compound relation within 'Jobs.'\n",
        "- **'Jobs'** functions as the prepositional object (pobj) for 'by.'\n",
        "- **'.'** operates as a punctuation mark (punct), indicating the end of the sentence.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mm0V7J6cz9UH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KguRvE1_0of4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}